# -*- coding:utf-8 -*-
# !/usr/bin/env python


import traceback
import os
import urlparse
from fuzz import fuzz
from basicinfo import _requests
from basicinfo import get_site_stander
from basicinfo import get_dict_by_server
from basicinfo import get_extion_by_sever


path = os.path.realpath(os.path.dirname(__file__))


def exploit_server_path(url):
    result = []
    try:
        standers = get_site_stander(url)
        r = _requests(url)
        dicts = get_dict_by_server(r.headers)
        if dicts:
            hand = fuzz(url, dicts, standers)
            result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_backup_path(url, dirs=[]):
    result = []
    try:
        domain = urlparse.urlparse(url).netloc
        standers = {
            "Content-Type": [
                "zip", "rar", "7z", "tar.gz", "tar.bz2", "tar", "trash"
            ]
        }
        preffix_path = os.path.abspath(os.path.join(path, "../dict/common/backup.preffixs"))
        suffix_path = os.path.abspath(os.path.join(path, "../dict/common/backup.suffixs"))
        if len(dirs) == 1:
            dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
            with open(dir_path) as f:
                commondirs = f.readlines()
            for i in commondirs:
                dirs.append(i.strip('\n'))
        with open(preffix_path) as f:
            preffixs = f.readlines()
        with open(suffix_path) as f:
            suffixs = f.readlines()
        dicts = []
        for d in dirs:
            for p in preffixs:
                p = p.strip('\n').format(host=domain)
                for s in suffixs:
                    s = s.strip('\n')
                    if d:
                        dicts.append('/' + d + '/' + p + s)
                    else:
                        dicts.append('/' + p + s)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_directory_path(url, dirs=[]):
    result = []
    try:
        standers = {
            "title": r"<title>([^<]*)"
        }
        dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
        with open(dir_path) as f:
            commondirs = f.readlines()
        dicts = []
        for i in dirs:
            dicts.append("/" + i + '/')
        for i in commondirs:
            i = "/" + i.strip("\n") + '/'
            dicts.append(i)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_common_file(url, extion, dirs=[]):
    result = []
    try:
        dicts = []
        standers = get_site_stander(url)
        files_path = os.path.abspath(os.path.join(path, "../dict/files.txt"))
        if len(dirs) == 1:
            #如果dirs 只有一个目录，说明没有解析到其他路径，从common/dirs.txt中枚举 目录
            dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
            with open(dir_path) as f:
                commondirs = f.readlines()
                f.close()
            for i in commondirs:
                dirs.append( "/" + i.strip('\n'))
        with open(files_path) as f:
            files = f.readlines()
            f.close()
        for d in dirs:
            for f in files:
                f = f.strip('\n').format(extion)
                if d:
                    dicts.append( '/' + d + '/' + f)
                else:
                    dicts.append('/' + f)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result
